Received: 2 May 2018
Revised: 24 July 2018
Accepted: 26 July 2018
DOI: 10.1002/smr.2108
RESEARCH ARTICLE ‐ METHODOLOGY
Multicriteria decision aid for agile methods evaluation using
fuzzy PROMETHEE
Omar El Beggar
LIM Laboratory—Advanced Smart Systems,
Computer Science Department, Faculty of
Sciences and Techniques of Mohammedia,
University Hassan II Casablanca, Casablanca,
Morocco
Correspondence
Omar El Beggar, Computer Science
Department, Faculty of Sciences and
Techniques of Mohammedia, Boulevard
Mohamed VI, MailBox 146 Mohammedia, PC
20650 Morocco.
Email: elbeggar_omar@yahoo.fr
Abstract
The last decade is marked by the blossom of agile methods and their growing use in
many IT projects. They shared common principles and values, such as customer col-
laboration, early and continuous delivery of releases, accept requirements changing,
and other common principles advocated by the agile manifesto since 2001. Mean-
while, there are significant differences among the practices adopted by the agile
methods to fulfill agility's principles and values. The differences could be interpreted
in terms of how an agile method judges the relative importance of such principles
or its orientation to a specific project viewpoint (organizational, technical, or concep-
tual) rather than others. This fact leads to a necessity of a deep evaluation of agile
methods to choose the best‐fit method suited for a project. Moreover, vague and
uncertain information used to evaluate those lightweight methodologies could repre-
sent an additional difficulty for decision makers (DM). To drawback this problem, the
multiple criteria decision aid (MCDA) method fuzzy PROMETHEE is proposed to
assess agile methods with regard to different conflicting criteria and handling besides
uncertainty in decision making. Furthermore, a sensitivity analysis is provided to val-
idate results and verify their stability.
KEY W ORDS
agile methods, decision making, fuzzy PROMETHEE, linguistic terms, MCDA
1
|
I N T RO D U CT I O N
Many agile methods are firstly used in industrial projects, but in the last decade, they are increasingly adopted for IT projects, and they know in
short term a sharp rise of interest and popularity as new software engineering methodologies. Promoting communication between project stake-
holders, customer collaboration, fast delivery at a steady pace of software, and acceptance of change are the principal values of agile software
development.1 Whereas the degree of compliance with these values differs from one method to another, as well as each one has its own practices
that might have effects on the project progress.
In fact, although agile methods present some interesting features regarding some project points of view, they have also some limitations com-
pared with other ones.2 For instance, the eXtreme Programming (XP) is more focused on technical practices rather than management practices
such as SCRUM method.3 XP is suitable for small and medium projects and cannot support large or complex projects like Feature‐Driven Devel-
opment (FDD).4 Besides, contrary to Scrum and XP which define responsibilities and roles in IT project, Kanban method does not.2
In other words, even as agile methods share common characteristics and principles advocated by the agile Manifesto5 in 2001, none of these
lightweight methods could be adopted for any kind of IT project. Silva et al6 claimed that according to the context in which the software will be devel-
oped, an agile method is more suitable than others. Therefore, one of the main challenges facing IT project manager is to make the decision of
selecting the appropriate agile method from several methodologies for a specific project. Furthermore, involving conflicting criteria to make a decision
might have significant consequences on project success as it is claimed by Silva et al.6 Hesari et al and Taromirad and Ramsin7,8 argued that a deep
J Softw Evol Proc. 2018;30:e2108.
https://doi.org/10.1002/smr.2108
wileyonlinelibrary.com/journal/smr
© 2018 John Wiley & Sons, Ltd.
1 of 21EL BEGGAR
2 of 21
assessment of the appropriateness of agile methods to their development environments is important due to their extensive number and diversity.
Thus, evaluation of agile methods could be considered as a decision‐making problem with qualitative and quantitative human assessments that should
be done. Further, this decision‐making problem could be more complex when assessments depend on subjective judgments under uncertainty.
To support this decision‐making problem, a specific MCDA approach which combines fuzzy set theory and PROMETHEE method is proposed.
In fact, the latter allows outranking alternatives or actions with respect to multiple conflicting criteria,9 while fuzzy set theory allows assessments
using linguistic variables and terms instead of crisp values.10 The fuzzy numbers associated with those linguistic terms will serve afterwards to
deduce crisp values. The choice of this MCDA method could be justified according to two arguments. Firstly, compared with other MCDA
methods qualified as compensatory methods, such as TOPSIS, MAUT, SAW, AHP, and MACBETH; PROMETHEE allows pairwise comparisons
with partial or limited compensations. In other words, the good scores obtained on certain criteria are not compensated by bad scores obtained
on others, so that some interesting solutions will not be ignored.11 Secondly, the chosen MCDA method could be easily understood and applied by
DMs, due to its ability to imitate the human mind in making preferences among alternatives in front of different conflicting criteria.12
In order to address the problem of selecting the appropriate agile method for a particular project, the proposal starts with the definition of
alternatives and criteria used in the evaluation of such software guiding methodologies. Next, a fuzzy decision matrix is established based on
the defined linguistic terms used for both criteria weighting and performance rating. Once, aggregated preference indices and flows are calculated.
The latter are defuzzified thereafter to rank alternatives (agile methods, to be more precise).
The rest of the paper is organized as follows: Section 2 presents the approach background, with an emphasis on the selected MCDA method
fuzzy PROMETHEE. Section 3 is devoted to a literature review. Section 4 presents the different steps of our approach's application for the eval-
uation of agile methods. Section 5 discusses the final results, and finally, section 6 presents the conclusions and suggestions for future works.
2
|
A P P R O A C H B A CK GR O U N D
|
2.1
General MCDA description
MCDA is a decision support discipline with a widespread usage in many areas.13 It aims to solve decision‐making problems that involve multiple
qualitative and quantitative criteria. In other words, MCDA methods have been developed to meet the DMs expectations by aiding him to make
the best decision facing a multiple alternatives or actions that are assessed against a set of conflicting criteria.
As stated by Silva et al,6 there are principally three kinds of problems treated by MCDA: determining a subset of alternatives considered the
best according to a defined set of criteria (choice), dividing alternatives into subsets with respect to some criteria (classification), and ordering alter-
natives from the best‐fit to the worst‐fit (outranking). Within MCDA context, different methods have been proposed to support the choosing,
classifying, and ranking of alternatives in multicriteria decision problems, such as Multi‐Attribute Utility Theory (MAUT),14 Analytic Hierarchy Pro-
cess (AHP),15 Simple Multi‐Attribute Rating Technique (SMART),16 and MACBETH.17 The aforementioned methods belong to the family of MCDA
which are based on a unique synthesis criterion calculated by aggregating performances of all alternatives into a unique performance function.
Those methods of total aggregation evacuate any possibility of incompatibility between alternatives.
On the other hand, the outranking approaches, such as PROMETHEE18 and ELECTRE19 consist of building an ordered list of alternatives
according to their evaluation on criteria, and the best ranked alternative is proposed as a solution.
The inconvenience of the unique synthesis criterion, as claimed by Zardari et al,11 is the inaccurate interpretation of results since some sig-
nificant performances on one criterion might be compensated by other ones which are trivial; consequently, some interesting solutions could
be dismissed. Nonetheless, outranking synthesis promotes pairwise comparisons for each single criterion with partial or limited compensation.
In this study, the chosen MCDA method is PROMETHEE which is a quite simple method to use in decision‐making problems compared with
other MCDA methods. Moreover, it imitates human thinking to make preferences in a finite number of alternatives assessed on the basis of sev-
eral conflicting criteria.12 Another advantage would be the ability of the method to support a large number of criteria and alternatives.20 Afful‐
Dadzie et al21 affirmed that PROMETHEE method is preferred than other outranking MCDA approach because of its robustness in comparing
alternatives performances and composite ranking.
|
2.2
2.2.1
Material and methodology
|
PROMETHEE method
PROMETHEE is the acronym of Preference Ranking Organization Method for Enrichment Evaluations; it was proposed the first time by Brans18
and improved in a number of variants or versions. The first version PROMETHEE I allows outranking alternatives in a partial order, avoiding thus
the balance between “strengths” represented by positive flows and “weaknesses” represented by negative flows. Although, the second version
PROMETHEE II realizes this balance by means of a complete outranking of alternatives based on their net flows. In this paper, we adopt the
two versions of PROMETHEE method to outrank a discrete set A of n alternatives represented by agile methods and assessed according to a finite
set C of m criteria:
A = {a1, a2, a3, …, an} a set of n discrete alternatives.
C = {c1, c2, c3, …, cm} a set of m conflicting criteria.EL BEGGAR
3 of 21
The evaluation matrix E(N × M) is built from f i(aj) that corresponds to a performance rating of an alternative aj with respect to a criterion i as
follows:
ðw1
wm Þ
⋯
c1 ⋯
cm
3
f 1 ða1 Þ ⋯ f m ða1 Þ
6
7 :
7
a2 6
6 f 1 ða2 Þ ⋯ f m ða2 Þ 7
6
7
6
⋱
⋮ 7
⋮ 4 ⋮
5
a1
an
2
f 1 ðan Þ
⋯
(1)
f m ðan Þ
Thereafter, the main objective of the decision‐making problem is maximizing performances of all alternatives with regard to the criteria set C, the
problem can be represented as follows:



max f 1 aj ; f i aj ; …; f m ðan Þ=aj ∈ A :
(2)
Furthermore, to perform PROMETHEE method, two entries are required:
• Weight coefficients to indicate the relative importance of criteria.
• Preference function that should be chosen by the DM to make preferences between alternatives performances
In fact, the preference function Pk usually expressed as a function P(dk) converts the difference dk between the evaluations f k(a) and fk(b)
obtained by two alternatives a and b according to a particular criterion k, into a preference degree comprises between 0 and 1. (0 indicates no
preference or indifference; 1 indicates a very strong preference). The following equations define the preference function:
Pk ða; bÞ ¼ Pðdk Þ ∀a; b ∈A; k ∈F;(3)
dk ða; bÞ ¼ f k ðaÞ−f k ðbÞ;(4)
0≤Pðdk Þ≤1:(5)
The preference of the alternative a with regard to b on all criteria could be calculated by means of a preference index as follows:
m
∏ða; bÞ ¼ ∑k¼1 wk Pk ða;bÞ
.
∑m
k¼1 wk
:
(6)
wk is the weight coefficient assigned to the criterion ck. Note that weights normalization ∑m
k¼1 wk ¼ 1 is not a necessity.
Finally, the ranking is performed by using positive and negative preference flows for each alternative. The positive outgoing flow indicates the
degree to which the alternative a dominates the other ones, while the incoming negative flow indicates the degree to which it is dominated by the
other alternatives.
ϕþ ðaÞ ¼1
∑ ∏ða; xÞ;
n−1 x∈A(7)
ϕ− ðaÞ ¼1
∑ ∏ðx; aÞ:
n−1 x∈A(8)
Later, the partial ranking of PROMETHEE I is performed through three relationships, preference P(I), indifference I(I), and incomparabilityR(I)
described below:
8 ðIÞ
aP b if Φþ ðaÞ>Φþ ðbÞ and Φ− ðaÞ<Φ− ðbÞ or
>
>
>
>
>
Φþ ðaÞ>Φþ ðbÞ and Φ− ðaÞ ¼ Φ− ðbÞ or
>
>
>
>
þ
þ
−
−
<
Φ ðaÞ ¼ Φ ðbÞ and Φ ðaÞ<Φ ðbÞ
:
þ
þ
−
ð
I
Þ
> aI b if Φ ðaÞ ¼ Φ ðbÞ and Φ ðaÞ ¼ Φ− ðbÞ
>
>
>
>
> aRðIÞ b if Φþ ðaÞ>Φþ ðbÞ and Φ− ðaÞ>Φ− ðbÞ or
>
>
>
:
Φþ ðaÞ<Φþ ðbÞ and Φ− ðaÞ<Φ− ðbÞ
(9)EL BEGGAR
4 of 21
On the other hand, the complete ranking performed by PROMETHEE II is based on the net flow that is calculated as follows:
ϕðaÞ ¼ ϕþ ðaÞ−ϕ− ðaÞ:
(10)
Thus, an alternative a is considered preferred or indifferent regarding an alternative b in PROMETHEE II as follows:
(
aPðIIÞ b if ðaÞ>ðbÞ
aIðIIÞ b if ðaÞ ¼ ðbÞ
:
(11)
Furthermore, there are further six basic types of preference functions were used in PROMETHEE: usual function, U‐shaped function, V‐
shaped function, level function, linear function, and gaussian function (see Figure 1).
2.2.2
The applied fuzzy PROMETHEE
|
Fuzzy PROMETHEE or F‐PROMETHEE is an extension of the PROMETHEE method that deals with the environment under uncertainty. This
extension is applied in different research areas, such as systems outsourcing, health care, logistics, and equipment.22-26 In this scope of problem,
usually, it is difficult to collect crisp data to perform evaluations in accurate way and make hence the appropriate decision. In fact, rating criteria
importance and performance of alternatives is considered a subjective task, with primarily hinge on human judgments and ill‐defined information.
For this reason, using fuzzy sets theory allows DMs solving the problem under data vagueness and obtaining a more realistic outranking of the
agile methods.
Within F‐PROMETHEE context, pair comparisons and calculations are made on fuzzy numbers. Yager in 1981 proposed an index of triangular
fuzzy number (TFN) denoted by Fe ¼ ða; n; bÞ and determined by the center of weights of the surface of its membership function. The membership
function μeðxÞ of a TFN is illustrated in Figure 2 and defined as seen in the Equation 12:
F
8
>
< ðx−aÞ=ðn−aÞ; a<x≤n
μeðxÞ ¼ ðb−xÞ=ðb−nÞ; n<x<b :
F
>
:
0; otherwise
(12)
On the other hand, fuzzy rating performed by DMs are given in linguistic terms such as, high, medium, and low. In fuzzy theory, as explained
by Celik et al,27 the concept of linguistic variables and terms used by fuzzy sets allows handling complex situations, where it is more suitable to
make judgments based on linguistic terms rather than crisp values. The fuzzy numbers can be defuzzified afterwards to crisp numbers using the
Yager index:
Fe ¼ ðn−a; n; n þ bÞ ¼ ð3n−a þ bÞ=3:(13)
Ff1 þ Fe2 ¼ ða1 þ a2 ; n1 þ n2 ; b1 þ b2 Þ;(14)
Ff1 −Fe2 ¼ ða1 −b2 ; n1 −n2 ; b1 −a2 Þ;(15)
Given two positive TFNs Ff1 ða1 ; n1 ; b1 Þ and Ff2 ða2 ; n2 ; b2 Þ, the following operations can be expressed between those TFNs as follows:
FIGURE 1The preference functions19
FIGURE 2An example of triangular fuzzy numberEL BEGGAR
5 of 21
λFf1 ¼ ðλa1 ; λn1 ; λb1 Þ; λ∈Rþ ;(16)
Ff1 ×Fe2 ¼ ða1 ×a2 ; n1 ×n2 ; b1 ×b2 Þ;(17)
Ff1 ÷Fe2 ¼ ða1 ÷a2 ; n1 ÷n2 ; b1 ÷b2 Þ:(18)
Drawing on those definitions and equations, the applied F‐PROMETHEE method could therefore be performed in five main steps as seen in
Figure 3. The details are as follows:
Step 0: It concerns a preprocessing step for definition of alternatives, groups of criteria, and guidelines to perform evaluation.
 
Step 1: Establishing a fuzzy decision matrix Ee
In this problem of agile methods evaluation under uncertainty, we establish a fuzzy decision matrix as described in the Formula (19). For this
purpose, we should define linguistic variables and their associated triangular fuzzy numbers for both performance ratings of alternatives and

e 1; w
e 1 ; …; w
e m ).
criteria weights. The fuzzy ratings ef i aj in the fuzzy matrix are expressed using linguistic terms or TFNs, idem for fuzzy weights (w
a1
2
e1
ðw
c1
ef 1 ða1 Þ
6
6
a2 6
6 ef 1 ða2 Þ
6
6
⋮ 6
6 ⋮
4
an ef 1 ðan Þ
⋯
⋯
⋯
⋯
⋱
⋯
e mÞ
w
cm
ef m ða1 Þ
3
7
7 :
ef m aj 7
7
7
7
⋮ 7
7
5
ef m ðan Þ
Step 2: Calculating aggregated fuzzy preference indices and fuzzy outranking flows
FIGURE 3
The proposed F‐PROMETHEE approach
(19)EL BEGGAR
6 of 21
To this end, the linear preference function (cf Figure 1) is applied with indifference and preference thresholds set at 0.1 and 0.3, respectively.
P¼
8
0
>
>
< d−q
>
p−q
>
:
1
d<q
q≤d≤p
(20)
d>p
The fuzzy preference degree of pairwise comparisons can be calculated by means of the Equation 21:
 


e
P¼p e
d ¼ P ef 1 −ef 2 ¼ Pðða1 ; n1 ; b1 Þ−ða2 ; n2 ; b2 ÞÞ
¼ Pða1 −b2 ; n1 −n2 ; b1 −a2 Þ
(21)
¼ ðPða1 −b2 ;Þ; Pðn1 −n2 Þ; Pðb1 −a2 ÞÞ:
Thereafter, fuzzy indices degree and fuzzy outranking flows could be computed as shown in the following equations:
m
e ða; bÞ ¼ ∑m w
e
e k;
∏
k¼1 e k P k ða; bÞ=∑k¼1 w
e ða; xÞ;
e þ ðaÞ ¼ 1 ∑ ∏
ϕ
n−1 x∈A
e ðx; aÞ;
e − ðaÞ ¼ 1 ∑ ∏
ϕ
n−1 x∈A
e − ðaÞ:
e ðaÞ ¼ ϕ
e þ ðaÞ−ϕ
ϕ
(22)
(23)
(24)
(25)
Step 3: Defuzzification of the fuzzy flows
This step consists of converting the fuzzy flows (entering, leaving and net) of each alternative (agile method), using Yager index in order to
perform partial and complete ranking and deduce thus the method that reaches the best compromise or balance between all criteria.
Step 4: Deep performance analysis through group criteria
This step consists of reproducing steps 2 and 3 to calculate fuzzy indices and flows but this time by groups of criteria. The reason behind is
comparing of agile methods with regard to a specific IT project viewpoint independently of others.
3
|
LITERATURE REVIEW
Many studies1,3,28-31 intended to propose generic comparisons of agile methods; almost all of them presented the scopes of methods, their asso-
ciated practices, activities, responsibilities, and the advantages that offered compared with conventional or traditional processes. The above
researches could serve for understanding various agile methods and determine similarities and differences between them. However, they remain
basic comparison studies and did not propose a deep evaluation. On the other hand, despite the benefits that present those lightweight methods,
many works in literature discussed furthermore some disadvantages or limitations of agile methods.2,32-34 For example, Harleen et al2 claimed that
agile methods are not always favorable, they also present some limitations, such as any small conflict might hurt the team collaboration, off‐site
customer, lack of technique or modeling skills and experiences, lack of documentation, large projects, manage distributed team, and many other
problems that could influence their applicability for any IT project.
Some studies attempted to address those limitations. For instance, Qureshi34 proposed an extended XP model to enhance the ability to man-
age large projects. A secure agile method was defined by Adelyar and Norta35 which considered “continuous integrations,” “planning game,” and
“pair programming” as security challenges that should be undertaken during software development life‐cycle (SDLC). Furthermore, Mahmud and
Abdullah36 suggested some adjustments to adapt agile methods for mobile development by examining mobile technology and application environ-
ment. In the same study, the Dynamic System Development Method (DSDM) was chosen as the best‐fit agile method that matches mobile devel-
opment. Another recent study carried out by Kropp and Meier37 highlighted the impact of human factors on agile methods and how teaching agile
values and practices could play an important role to be correctly applied in industry.
The majority of the above comparative studies have been convinced that the choice of the appropriate method is a critical decision that
increases the chances of the project stockholders in the success or failure of their software development. Silva et al6 stated that depending onEL BEGGAR
7 of 21
the context in which the software will be developed, an agile method is more appropriate than others. Besides, to select an agile method from
among different existing methods is often made according to a set of conflicting criteria, where the methods do not have only good ratings on
them. Therefore, the decision becomes very complex and might have significant consequences. For example, XP is more focused on technical
practices and provide greatest software quality, but at the same time, it is not destined for large projects and teams.
The latter consequences were discussed by Dyck and Majchrzak.38 They have specified as well a set of parameters to select a suitable meth-
odology by proposing a classification approach. However, as it is claimed by the authors, the project characteristics and their associated criteria
could add an extra complexity to the classification. Due to the conflicting criteria related to a project context, Hesari et al7 and Taromirad
et al8 have persuaded that a deep evaluation of the agile methods is required. Besides, Harb et al39 stated that selecting the best agile method-
ology is a decision‐making problem under multiple conflicting criteria. Generally, to the best of our knowledge, in literature, few of researches
were interested in MCDA to perform evaluation of agile methods. Harb et al39 proposed an MCDA framework based on AHP to evaluate four
methods (Crystal, FDD, Scrum and XP) according to 10 criteria. The method is based on pairwise comparisons of alternative and criteria, and
the resulting ranking is XP > Scrum > FDD > Crystal. Commonly in AHP methods, DMs are invited to perform both pair comparisons of alterna-
tives on each criterion, and pair comparisons of criteria to deduce their proper weights. When a huge number of criteria and alternatives are
involved, the DMs are overworked by making a lot of comparisons and therefore their patience to continue those comparisons could be lost dur-
ing the process.18 Another criticism for AHP concerns inconsistent DMs judgments that could often cause the problem of preferences intransitiv-
ity.40 Conscious of this problem, Demirtas et al41 proposed an MCDA approach by combining SWOT with fuzzy AHP. In fact, criteria have been
defined using SWOT technique, and their associated weights have been provided using fuzzy AHP. The evaluation is performed on waterfall and
agile methodologies against 13 criteria.
On the other hand, Silva et al6 proposed the SMARTER method for evaluating four agile methods (Crystal, DSDM, SCRUM, and XP)
with regard to 13 criteria. The proposed MCDA is based on Swing and ROC techniques to weigh criteria. The obtained final ranking is
XP > Scrum > Crystal > DSDM. Nevertheless, the authors affirmed that the level of imprecision is very high and might thus impact their results.
A new MCDA approach was proposed by Vavpotič and Vasilecas.42 The proposal included 11 criteria divided into two groups: project proprieties
and method proprieties, while the evaluation is based on a normalized value so‐called Methodology‐Project‐Value, and calculated for six methods:
SCRUM, Test Driven Development (TDD), XP, RUP, RUP for small projects, and Oracle Custom Development Method (CDM). The study con-
cluded that RUP for small projects is the more suitable method. However, the approach lacks from techniques to assign importance for criteria
or make a prioritization of them.
Besides, Taromirad and Ramsin8 proposed an MCDA framework so‐called CEFAM with 13 criteria which belong to five groups: process,
modeling language, usage, agility, and cross context. The XP is the only method evaluated using CEFAM. In this work, the authors did not specify
any technique for performances rating of alternatives and criteria weighting.
As a synthesis, most of these studied MCDA approaches are compensatory methods and suffer from the absence of techniques that treat
vagueness or uncertainty in DMs judgments, which could thus influence their results. And finally, except for Demirtas et al41 and our proposal that
is based on GQM pattern that will be presented in the next Section 4.2.1, the abovementioned approaches did not support neither a methodology
for building the criteria nor a sensitivity analysis to validate their proposed models. The Table 1 summarizes the main differences of those studies
with our approach.
Concerning the MCDA approaches conducted via fuzzy PROMETHEE, there are also several recent studies of different sectors available in
the literature. Gul et al43 applied fuzzy PROMETHEE for solving material selection problems, while the study of Ozsahina et al44 aims to evaluate
the most common nuclear medicine imaging devices. Always in health sector, intuitionistic fuzzy PROMETHEE with entropy measure are used by
Pratibha and Divya.45 In environment, many recent studies were found (Tabaraee et al46 and Lolli et al47). Fuzzy PROMETHEE is also applied in
logistics and transport (Elevli48), IS outsourcing (Chen et al22), software engineering (Boutkhoum et al49), and many other sectors.
Another work50 combined fuzzy AHP and fuzzy PROMETHEE in order to propose an empirical framework to evaluate agile methods accord-
ing to the requirement of a particular project. This study suffers from many drawbacks; in fact, it suffers from lack of criteria to cover all important
aspects in evaluating agile methods. The authors proposed only four criteria (reluctance to change, documentation and formalization, process
overhead, reliability and scalability), which are insufficient and do not respect the criteria axioms or requirements.51 For example, the criterion
TABLE 1
Comparison of MCDA approaches for evaluating agile methods with our proposal
CriteriaHarb et al39
Demirtas et al41
Silva et al6Taromirad et al8Vavpotič et al42Our proposal
MCDA methodAHPSWOT‐fuzzy AHPSMARTERCEFAMMPV methodPROMETHEE
MCDA typeCompensatoryCompensatoryCompensatory‐CompensatorySemi‐compensatory
Alternatives count424168
Criteria count101313131120
Weighting techniqueAHPFuzzy AHPSwing/ROCNot supportedNot supportedFuzzy logic
Uncertainty coverageNoYesNoNoNoYes
Criteria building methodNoSWOTNoNoNoGQM
Sensitivity analysisNoNoNoNoNoYesEL BEGGAR
8 of 21
documentation and formalization or the criterion reliability and scalability could each one measures more than one project aspect, which is depre-
ciated in MCDA problems. The authors even acknowledge that there is no guarantee that these criteria are those that are required for the eval-
uation. Moreover, no guidelines are presented to show neither how pairwise comparisons between alternatives are made nor between criteria
importance to accomplish performance decision matrix and calculate thus the resulting indexes and flows via PROMETHEE.
In conclusion of the literature reviews abovementioned, to the best of our knowledge, no comprehensive study has applied the fuzzy
PROMETHEE method to evaluate agile methods according to the their practices and principles to fulfill agility values, and no study has presented
a comprehensive evaluation of agile methods in front of groups of criteria, constituting critical project viewpoints.
4
|
CASE STUDY USING OUR F‐PROMETHEE
At the present work, we apply the F‐PROMETHEE for the selection of the best‐fit agile method from among eight lightweight methodologies.
Thus, we start by setting up the problem through defining alternatives, criteria, and guidelines for the evaluation of the alternatives. Next, we
determine the relative importance of each criterion and alternative scoring using linguistic terms, in order to calculate preference indices and
outranking flows. Finally, resulting partial and complete order of agile methods are performed.
|
4.1
Definition of alternatives
Interestingly, the study concerns eight widely used agile methodologies that have been introduced and developed firstly: XP by Beck 1999, Scrum
by Schwaber and Beedle 2001, Kanban by Anderson 2003, Dynamic Systems Development Method (DSDM) by Stapleton 1997, Crystal Clear by
Cockburn 2001, FDD by Ambler 2002, Agile Modelling Driven Development (AMDD) by Ambler 2002, and Adaptive Software Development (ASD)
by Highsmith 2000. The choice of those methods is arbitrary, and it was not influenced by their popularity ranking or rate of using as it was pub-
lished recently in the 11th annual report of agile survey.52 The above agile methods are considered as alternatives for our MCDA approach.
|
4.2
4.2.1
Definition of criteria and linguistic scales
|
Criteria building
Our method adopted to build criteria is inspired from Goal/Question/Metric (GQM) paradigm53 which is a relevant mechanism to define goals and
generate underlying questions allowing their assessment. The first step in building criteria consists of defining goals according to the values, prin-
ciples, and practices advocated by the agile manifesto. In fact, the agile manifesto provides four main values and 12 principles,5 as well as a set of
practices54 performed by the agile methodologies in the software development.
Using the agile manifesto as a guide, we have attempted to tidy the principles by their related values and likewise practices by their corre-
sponding principles. The objective is to define the goals that each group of principles and associated practices intended to achieve. We also rec-
ognize that different practices might be used to realize the same principle. According to GQM pattern, we defined six goals which are G1:
“Enhance communication and collaboration between project members,” G2: “Produce regularly and rapidly operational software,” G3: “favor
essential design and documentation,” G4: “Improve software quality and generation,” G5: “Increase customer involvement and feedback,” and
G6: “Hold a flexible process and planning during the project”.
From the defined goals, a set of derived questions would determine how goals could be met. Afterwards, from the underlying questions, a set
of indicators or (criteria) are built. Each criterion is designed to measure a particular goal reflective of agility principles and practices. For example,
the goal G1 requires asking the following questions: (1) How teams are organized? (2) What channels used by agile methods to ensure team com-
munication? (3) How they collaborate collectively? Those questions allow deducing five organizational criteria that need be assessed: team size,
definition of responsibilities, distributed/collocated team issued from the first question. Formal communication issued from the second question,
and collective ownership is deduced from the third question.
In concert with the above, there are 20 criteria derived from the defined goals that help accomplish the evaluation of agile methods, as seen in
Table 2. Moreover, we note that the systematic review of the literature6,28,30,31,39,42,55 has revealed that almost all of those derived criteria are
used in agile methods comparison. We recognize, however, that agile methods might propose over time new practices; in this case, new goals
can be defined and thus new assessment criteria will be added to our model.
On the other hand, criteria used in decision‐making problems should be selected in accordance with the following requirements51:
• Completeness: All important criteria relevant to the decision should be included and cover every critical aspect of the problem.
• Nonredundancy: Unnecessary or unimportant criterion should be eliminated. A criterion is considered also redundant when all alternatives
have likely the same evaluation according to it. The latter should be removed from criteria set, since it will have no impact on the final decision.
However, omission of such criterion should be made carefully because this might affect the reliability of the analysis, especially when new
options would be under consideration with different performance level on this criterion. DMs may include it in such situation.
• Nonoverlap: The defined criteria should avoid double counting, ie, criteria which assess the same dimension attribute. Otherwise, we will give
too much importance to this dimension attribute more than it deserves. Interestingly, we must not confuse overlapping and correlation.EL BEGGAR
TABLE 2
The corresponding goals and criteria of the agility philosophy derived using GQM pattern
Corresponding Goals and Criteria
(GQM pattern)
Agile Philosophy (Manifesto)
ValuesPrinciplesPracticesGoalCriteria
Individuals and interactions over
processes and toolsMotivated individuals.
Face‐to‐face conversation.
Self‐organizing teams.
Stakeholders work togetherSelf‐organizing team
Cross‐functional team
Collective code ownership
Pair programming
Collocated team
Daily progress meetings
Face‐to‐face communication
Using metaphorG1: Enhance communication and
collaboration between project
members• Formal communication
• Distributed team
• Definition of responsibilities
• Team size
• Collective ownership
Working software over
comprehensive documentationRegular releases in shorter timescale.Iterative and incremental development
Just in time (JIT)
Iterations smaller and frequent releases
Work 40 hours a week
Minimizing big design up front (BDUF)
CRC modeling
Simple and interactive design
Responsibilities‐driven design
Agile documentation
Test first development or design (TFD)
Continuous integration and deployment
Acceptance test
Automating test build
Automating software build
Automating documentation generation
Refactoring
Avoiding anti‐patterns
Risk analysis
Prototyping
Adherence to coding standardsG2: Produce regularly and rapidly
operational software
G3: Favor essential design and
documentation• Cadence of short development cycles
Simplicity
Working software is the primary measure
of progress.
Sustainable development.
Technical excellence and good design
G4: Improve software quality and
generation
• Emphasis on software modeling
• Modeling coverage of generic software
aspects
• Appropriate documentation volume
• Smoothness of transition from model to
code
• Explicit risk mitigation
• Preventive control of programming quality
• Definition of SQA standards
Customer collaboration over
contract negotiationCustomer satisfactionClient‐driven development
Collocated customers
Continuous feedback
Review meetingG5: Increase customer involvement
and feedback• System review opportunities
• Client implication
Responding to change over
following a planEmbrace requirements changing
Frequent reflection and improvementRetrospective meetings
Reflection workshop
Just rules
Project size estimation and agile planning
Planning poker
Measure team velocity
Prioritization of user stories
Time boxing
Tracking and enhancing project progressG6: Hold a flexible process and
planning during the project• Process's SDLC coverage degree
• Improvement of approval process
• Changes acceptance
• Planning prioritized sprints
• Suited project size
9 of 21EL BEGGAR
10 of 21
Indeed, criteria could be correlated while still assessing different goals. So that, the criteria which measure the same thing should be
consolidated into a composite criterion.
• Preference independence: The performance scoring on a criterion should not depend on the performance scoring of other criteria. To deter-
mine either a criterion is preference independent of the others, the answer of the following question should be yes: Could we score the alter-
native on one criterion without knowing what is its scoring on any other criteria?
The 20 criteria used in this decision‐making problem are tested on these requirements, we could ensure that all criteria are preference‐
independent and all critical aspects were captured without overlapping or redundancy.
4.2.2
|
Criteria hierarchy
This subsection is devoted to constructing a criteria hierarchy and defining the linguistic terms and associated fuzzy numbers that will be used for
performance scoring on criteria. Firstly, the criteria have been divided into four categories or viewpoints that are organization, technique, model-
ing, and software quality assurance (SQA).
In fact, the organization category is interested in the project environment (project size, team size, roles, etc). The technique viewpoint meant
evaluating agile methods with regard to their technical practices and features. The SQA56 concerns the preventive procedures or practices des-
tined to control conformance of product and process with customer needs. Notwithstanding, agile methods do not pay a great attention to model-
ing and modeling language, a modeling group of criteria was proposed, since models are often still important for developing software, even though
partially or indirectly.8 The four categories encompass the previous 20 defined criteria, which are distributed according to the context that they
aim and organized in a hierarchy as we can see in the Figure 4.
On the other hand, the performance scoring of alternatives according to criteria will be made on the basis of the linguistic terms and their cor-
responding TFN of the Table 3. It is important to note that two families of linguistic terms are used for the same TFN to perform appropriate scoring.
For instance, we can assign the best evaluation according to the criterion “team size” using very large. However, very good might be assigned as the
best scoring in front of a criterion such as “client implication.” Interestingly, the chosen linguistic scale is comprised between 0 and 1 to keep the
membership of the values returned by preference functions to this interval, elsewhere a normalization is needed for the fuzzy rating.
Table 4 presents the list of criteria and the type of linguistic scale employed for the evaluation.
4.2.3
|
Criteria definition and guidelines for evaluating alternatives
The next paragraphs will exploit the different studies in the specialized literature focused on comparing and evaluating agile methods, in order to
gather data that serve as an evaluation guidance for DMs.
Client implication (O1) points out the direct and close client involvement in the project. In fact, XP suggests on‐site customer,2 the client is
directly involved in almost activities, all time, and throughout the different SDLC steps. XP could hence obtain the top scoring according to this
criterion. On the other hand, although the client involvement is imperative for the other agile methods,5 they never recommended on‐site cus-
tomer. Indeed, the customer is involved in different ways, through either the product owner for Scrum, or reports for FDD, or frequent releases
for Crystal Clear, ASD, DSDM,28,39 and finally, through models for AMDD.57 Therefore, the rest of agile methods could obtain a good scoring, but
less than XP.
FIGURE 4 Criteria hierarchy used to assess
the agile methods
TABLE 3
The used linguistic scales and their associated triangular fuzzy number (TFN) for performance rating of alternatives
Linguistic Term (Type 1)TFNLinguistic Term (Type 2)
Very good (VG)(0.9, 1, 1)Very large (VL)
Good (G)(0.8, 0.9, 1)Large (L)
Slightly good (SG)(0,6, 0.7, 0.9)Moderately large (ML)
Fair (F)(0.5, 0.6, 0.7)Medium (M)
Slightly fair (SF)(0.3, 0.4, 0.6)Moderately medium (MM)
Poor (P)(0.1, 0.3, 0.4)Small (S)
Very poor (VP)(0, 0, 0.2)Very small (VS)EL BEGGAR
11 of 21
TABLE 4List of criteria and related linguistic termCriterionDescriptionO1Client implication1
O2Team size2
O3Definition of responsibilities1
O4Distributed team1
O5Formal communication1
O6Suited project size2
T1Planning prioritized sprints1
T2Cadence of short development cycles2
T3Changes acceptance1
T4Explicit risk mitigation1
T5Appropriate documentation volume2
T6Collective ownership1
T7Process's SDLC coverage degree2
M1Emphasis on software modeling1
M2Modeling coverage of generic software aspects2
M3Smoothness of transition from model to code1
Type of Linguistic Terms
Q1Preventive control of code quality1
Q2System review opportunities2
Q3Improvement of approval process1
Q4Definition of SQA standards1
Team size (O2) indicates whether agile method supports small, medium, large team, or all sizes alike. Interestingly, an agile method should sup-
port a large team without making projects less agile or being in contradiction with the agility values and principles. The Scrum team is best suited
for team less than 10 persons.55 Abrahamson et al4 suggest a team composed of five to nine members, but it could be scaled to a large number as
it is stated in 32,55 (Scrum of Scrums). XP could support small and medium sized teams4,39 (less than 20 persons). Flora and Chande2 precise that
the XP team size is from two to 12 members and not scalable to multiple teams. Team size in DSDM varies from two to six persons with the pos-
sibility to have many teams working on different components of the split system.2,4,55 Crystal Clear is for very short‐term projects (up to six devel-
opers).2 No specific team size is revealed for Kanban and AMDD, but the methodologies were tailored for small teams.2 FDD and ASD could
support from small to large teams.4,39 The evaluation takes into consideration only the upper team size supported by an agile method. Hence,
team size for both methods FDD and ASD is rated large. Due to the possibility of extension, Scrum and DSDM teams are also rated large. How-
ever, the upper sized team of XP is medium; Kanban and AMDD aimed for small sized teams at most, compared with the upper team size of Crys-
tal Clear, which is considered very small.
Definition of responsibilities (O3) specifies whether responsibilities or roles in agile methods are well detailed or not. The methods Scrum, XP,
ASD, DSDM, Crystal Clear, and FDD define project roles such as product owner, scrum master, and development team for Scrum.3 XP in turn, the
roles client, tracker, coach, manager, programmer, tester, and consultant are defined.8 ASD presents the following roles: executive sponsor, par-
ticipants, developer, and customer,2,4 while DSDM defines 15 roles: executive sponsor, visionary, ambassador, advisor, project manager, technical
coordinator, team leader, solution developer, tester, scribe, facilitator, and other specialist roles.2,4 On the other hand, senior designer/program-
mer, designer/programmer, sponsor, and user are the master roles defined for Crystal Clear,2,4 with the possibility to define further seven subroles
that can be assigned to the master roles. Furthermore, FDD defines six key roles: project manager, chief architect, development manager, chief
programmers, class owners, domain experts; it could further involve eight supporting roles.2,4 However, Kanban and AMDD did not define any
specific project responsibilities.2 In concert with the above, given that project roles are very well detailed for FDD, DSDM, and Crystal Clear, their
scoring according to this criterion is the best. Next, XP got a good score, while Scrum and ASD obtained an equal fair score. However, Kanban and
AMDD have the worst performance rating on this criterion.
Distributed team (O4) refers to using colocated or distributed teams in agile methods. Although colocated team is recommended in agile phi-
losophy, it is sometimes necessary in the manufacturing of complex systems to ask for support from an external expertise or consultant. As well in
outsourcing projects, the professionals located in remote geographic areas should work together. Hence, the collocation should not be an
enforced practice. In fact, unlike the other studied methods, ASD tolerates the use of distributed teams through techniques that support such sit-
uation.4 However, given that the rest of the agile methods are oriented colocated teams,30 they should be adaptable to support also distributed
teams, provided that they must cede some of their practices or activities. Therefore, except for ASD which got the best performance rating, the
remainder of methods got a fair score.
Formal communication (O5) examines the formal style and the channels proposed by agile methods to improve communication during the pro-
ject. In fact, ASD, DSDM, FDD, and Crystal Clear propose special workshops often held in open work areas, such as Join Application Development12 of 21
EL BEGGAR
(JAD) sessions in ASD,28 or reflection workshops in Crystal Clear.4 Workshops are occasions to discuss work and enhance collaboration between
project members. Likewise, XP and Scrum propose daily celebrities to communicate project progress face‐to‐face, such as stand‐up and daily
meetings.2 All the above methods specify further specific roles that are responsible for facilitating communication. Moreover, XP claimed using
metaphor8 by programmers to simplify the comprehension of any complex concepts to customer as well as pair‐programming3,8 to enhance assis-
tance between programmers. Nevertheless, Crystal Clear suffers from limitations in its osmotic communication structure (ie, the principle of the
ambient diffusion of information with passive listening).4 On the other hand, Kanban promotes wide communication principle without specifying
how to be performed.4 Notwithstanding storming or modeling sessions proposed by AMDD, it presents an informal style of communication since
it just suggests an open and honest communication,57 without specifying techniques to ensure it. Therefore, the latter methods Kanban and
AMDD presented a poor formal communication, while the other methods are better, especially XP.
Suited project size (O6): Crystal Clear is suitable for very small projects.4,32 XP, Scrum is preferred for small and medium projects needed to be
done quickly.8,31,55 However, FDD and ASD are suitable for large and complex projects.2,55 The remainder of agile methodologies DSDM, Kanban,
and AMDD are convenient for all project sizes.2,4 It is important to note that the performance rating of alternatives corresponds to the maximum
suited project size for each method.
Planning prioritized sprints (T1) indicate whether the agile methodology supports a planning phase that schedules prioritized sprints according
to their estimated cost and time. Indeed, the majority of agile methods requires estimation of project time and cost based on team velocity and
complexity of user stories. For instance, Scrum uses the release, product, and sprint backlogs.3 XP and AMDD use planning game, which is often
based on the technique of planning Poker.2,34 DSDM requires a feasibility phase to assess cost and time for delivery.2,4 FDD uses a feature planning
which schedules the prioritized features according to different milestones.2,4,55 Likewise, planning cycles using counts of function points is indis-
pensable for ASD.4 The planning of the next release is required in the staging phase of Crystal Clear. As well before project starting, the Crystal
family suggests determining the project criticality and size to decide on the color relative of the methodology heaviness.4 However, Kanban man-
ages and controls visually the team working rather than sprinting and plan sprints. So at first, no durations are predetermined for sprints, but only
limits of Work In Progress (WIP) are fixed based on the effort of tasks that are done. The main Kanban goal is to avoid overloading and excess of the
team capacity.1,2 Thereby, only Kanban method has a fair score according to this criterion. The other methods are judged much better.
Cadence of short development cycles (T2) assesses the cadence of short development cycles or iteration produced by an agile method. The
key idea is short iteration length implies large cadence and productivity. In fact, the FDD method takes a few days to 2 weeks at the most to per-
form iteration or sprint.4 XP iteration2 is between 1 and 3 weeks. SCRUM spends 2 weeks up to 1 month to finish a sprint.2,31 However, ASD
sprint4 is from 4 to 8 weeks, and Crystal Clear's incremental development cycles2,4 are up to 4 months. The iteration length in AMDD, DSDM,
and Kanban depends on the project size.2,4 Given the highest duration that can tolerate the agile method for sprinting, XP and FDD are considered
methods with very large cadence. Scrum with a medium cadence, while ASD and Crystal Clear are considered methods with respectively small and
very small cadences. Finally, to avoid penalizing the methods AMDD, DSDM, and Kanban, since their scoring on the previous criterion “project
size” has been set to large. The medium cadence was then assigned to them.
Changes acceptance (T3) evaluates the method acceptance or reciprocally resistance to changes. Due to some milestones or time‐boxing
imposed by the lightweight methods, changes or improvements are not allowed on items of the sprint when it is started (closed time‐boxing).1,31
For this reason, Scrum, FDD, DSDM, ASD, and Crystal Clear could be considered more resistant to changes than XP, AMDD, and Kanban.4 There-
fore, the latter methods got the best rating on this criterion, while the remainder of methods got a fair rating.
Explicit risk mitigation (T4): The criterion investigates the ability of agile methods to reduce risk by proposing explicit activities or practices.
Walczak and Kuchta argued that agile methods often mitigate risk implicitly through the small development cycles, quick feedback, customer
involvement, etc. However, explicit methods of risk management are often still required.58 For instance, the pre‐game phase in scrum consists
of risks assessment,2 while the feasibility study in DSDM includes risk analysis and a fast prototyping before to proceed to the next phase or
not.4 Crystal family recommends, as it has previously cited, choosing the appropriate color of the methodology according to the specified project
size and criticality. ASD is a risk‐driven method with an emphasis on software prototypes as stated by Abrahamson et al..4 According to the same
study, Scrum, Crystal Clear, DSDM, and ASD could be considered methods with high explicit risk mitigation; they got thus the best score on this
criterion. Meanwhile, the other agile methods are judged as methods with quite explicit risk mitigation.
Appropriate documentation volume (T5): Taking into account the crucial role of documentation throughout the SDLC, especially appropriate
documentation which still required even in agile methods. The authors in34 advocated that appropriate documentation in agile methods helps the
team to evolve and re‐engineer its software. In light of this consideration, the present criterion assesses the documentation volume supported by
agile methods. For instance, FDD and DSDM claimed a large volume of appropriate documentation, such as reporting of results and information
reporting for FDD,2 while DSDM4 requires documents such as feasibility report, development outline plan, user manual, risk analysis document,
etc. On the other hand, the remainder of agile methods provides a basic documentation28,39 (medium scoring on the criterion).
Collective ownership (T6): XP recommended the pair‐programming with a cyclic team assignment.3,8 In other words, two programmers work
together at a single workstation, and any team member could make changes to any code. Modeling with others and prove it with code, is the prac-
tice adopted by AMDD to satisfy collective ownership.57 However, the FDD method is based on individual class ownership (set of features
assigned to a unique programmer).2,4 The rest of agile methods are supposed to support partially the collective ownership, since they encouraged
collaborative and cooperative work in self‐organizing and cross‐functional teams.29,30 Hence, the best performance rating is attributed to XP and
AMDD. Next at a rank tied, we found Scrum, ASD, Crystal Clear, DSDM and Kanban with a fair score, while FDD at the last rank got a poor score.EL BEGGAR
13 of 21
Process's SDLC coverage degree (T7): Based on the analysis study performed by Abrahamsson et al,59 this criterion specifies whether the
generic phases of the SDLC (inception, analysis, design, code, unit test, integration test, system test, acceptance test, and system in use) are cov-
ered by the following method aspects: project management, process, and practices. Firstly, the referenced study did not integrate a crucial practice
of XP and AMDD, namely, the TFD that allows covering the different kinds of tests. Besides, it is also important to distinguish between the treated
approach in the study Agile Modeling (AM) which provides a set of the good agile modeling practices and AMDD that is an agile method.
Thereafter, in order to assess the criterion in a quantitative way, a coverage rate is calculated according to the following equation:
cov ¼
1 N 1 9
∑ ∑ ua;p , where N ≤ 3 indicates the number of aspects supported by the method and the value of ua, p is determined as follows:
3 i¼1 9 i¼1

ua;p ¼ 1 when the supported aspect a of the method covers the phase p
ua;p ¼ 0 elsewhere:
For example, the SDLC coverage rate of XP is calculated as follows:
covðXPÞ ¼ ð1=3ð0 þ 1 þ 1 þ 1 þ 1 þ 1 þ 1 þ 1 þ 0Þ=9 þ 1=3ð0 þ 1 þ 1 þ 1 þ 1 þ 1 þ 1 þ 1 þ 0Þ=9Þ:
covðXPÞ ¼ 0:52:
The rest of the SDLC coverage rates of Kanban, Crystal Clear, Scrum, FDD, ASD, AMDD, and DSDM are 0.26, 0.30, 0.33, 0.37, 0.44, 0.48, and
0.86, respectively. Finally, the adopted scoring of alternatives on this criterion is performed on the basis of the Table 5.
Emphasis on software modeling (M1): FDD, DSDM, and AMDD promote modeling during the realization of increments. FDD and DSDM
encompass specific design phases. In fact, modeling in FDD is performed using UML diagrams during “develop an overall model” and “design
by feature” phases,2 while “functional model iteration” and “design and build iteration” are modeling phases for DSDM.4 On the other hand,
AMDD is a modeling‐driven method focused mainly on depicting models before the code and deducing the latter from them.57 The rest of agile
methods presents a quite emphasis on software modeling. Thus, AMDD got the best score, followed by DSDM and FDD, while the other methods
obtained equal fair score.
Modeling coverage of generic software aspects (M2) checks which system's generic aspects (architectural, structural, behavioral, and func-
tional) are covered by agile modeling. In fact, AMDD aims to cover modeling of any system aspect.57 DSDM models cover architectural, behav-
ioral, and functional software aspects, while FDD's object domain modeling supports only the structural and functional aspects.4
The other methods did neither made much attention to modeling nor specifying any particular type of model,8 but some agile models are used
when it is needed. Therefore, AMDD and DSDM got so the upper score, followed by FDD, and finally the rest of methods got a medium score.
Smoothness transition from model to code (M3) specifies whether prescribed techniques exist for providing a smooth transition from model to
code, reducing thus the gap between them and ensure small cycles of development. Unlike the other agile methods, AMDD and FDD claimed
using some techniques to the smooth transition between model and code. In fact, AMDD could use Model‐Driven Architecture (MDA)60 para-
digms such as transformation to ensure transition as it is discussed by Ambler.61 On the other hand, FDD proposed “Walk‐through” technique,4
which serves as a guide for the development team to move from the overall domain model to subdomains models and next providing an object
model to build associated features. There are no evidences in the other agile methods which specify how to ensure smoothness of this kind of
transition. Thus, AMDD got the upper score, followed by FDD, and thereafter the other agile methods with a fair score.
Preventive control of code quality (Q1) verifies how an agile method allows preventive monitoring and control of quality from the beginning of
the software development. For example, XP introduces code standards, code refactoring to avoid anti‐patterns, pair‐programming, and Test First
Development (TFD) to enhance technical quality and improve design code.38 Likewise, AMDD avoids design smells or anti‐patterns using code
and database refactoring, as well as TFD and modeling standards.57 The remaining methods suffer from lack of technical practices dedicated to
this purpose. Nevertheless, given that tests are mainly recommended in agility, the rest of agile methods integrate also different kinds of tests
(unit, regression, integration, and acceptance) and their automation throughout the SDLC.33 However, they did not perform them earlier before
TABLE 5
Alternatives scoring according to the SDLC coverage rate
SDLC Coverage Rate ScaleScoring (Linguistic Term Type 2)
Cov rate > = 0.8VL
0.6 < = cov rate < 0.8L
0.5 < = cov rate < 0.6ML
0.4 < = cov rate < 0.5M
0.3 < = cov rate < 0.4MM
0.2 < = cov rate < 0.3S
Cov rate < 0.2VSEL BEGGAR
14 of 21
code like in XP and AMDD. Therefore, XP and AMDD got the upper performance rating on this criterion, contrary to the other methods which are
judged with poor preventive control of code quality.
System review opportunities (Q2) determine whether the method supports frequent system review's opportunities to fix any found defects or
discordance with customer needs before delivery. Scrum requires a review meeting at the end of each sprint to verify and validate sprint's prod-
uct.39 FDD is interested in software inspections and reviews of design and code, which is often conducted before the design/code of the next set
of features.4 Quality review in ASD occurred at the “learn” phase consists of presenting developed functionalities to users for review.28
Furthermore, design and functional prototypes in DSDM are reviewed by users, as well as a business review is performed when the system is
transferred from the development environment into the production environment.4 On the other hand, Kanban suggests feedback loops as various
software review stages.2 Crystal Clear requires a revision and review of the objectives of the increment,4 while XP is based on continuous system
reviews and quick customer feedbacks.2 With the exception of XP, the above agile methods provide commonly system review at the end of the
increments, when the associated functionalities are completely developed. In contrast, in XP each new piece of code is usually reviewed and inte-
grated into the system as soon as it is possible many times a day.
However, within AMDD method, model reviews and even code inspections are optional as it is claimed by Ambler57; the author argued that
the value of reviews quickly disappears when the model is mapped to code in the same day. In our evaluation, the latter method got thus the worst
score, while the other agile methods obtained a medium rating and particularly XP awarded the best performance on this criterion.
Improvement of approval process (Q3): Kanban tolerates a very high flexibility in process adjustment since it is based on visual workflow with
WIP limits to manage and adapt the process and make policies explicit.2 On the other hand, Scrum suggests retrospective meetings39 to discuss
problems and learn lessons in order to improve the working process in the future sprints. A sprint goal is further defined beforehand in Scrum to
make necessary changes if some process troubles are detected. Besides, the so‐called “just rules” XP practice4 implies that followed rules could be
changed any time if a common agreement is made, and XP process could be adjusted therefore according to the new customer needs. Besides, the
final Q/A and release stages in ASD allow capturing learned lessons and make adjustments.4 In DSDM, time‐box retrospective workshop is the
ideal opportunity to learn from the previous time‐box and improve hence the future time‐boxes.2 The Crystal Clear practice “methodology‐tuning”
consists of gaining knowledge to improve process by means of user interviews and team workshops.4 There are no evidences of how AMDD tol-
erates learn and process improvement. Consequently, the AMDD got the lowest rating contrary to the other methods, especially Kanban, which
got the upper score on the current criterion.
Definition of SQA standards (Q4) verifies the existence of guidelines (standards) to which software products are compared. According to
SenthilMurugan and Prakasam,56 there are three types of SQA standards: (documentation standard, design standard, and code standard). Indeed,
Crystal Clear encompasses various templates that cover the three standards,4 while XP and AMDD support only code standard and design or
modeling standard.2,4,57 However, no standards were specified for the remainder of the studied agile methods. Thus, Crystal Clear got the upper
score, followed by XP and AMDD, and thereafter the other agile methods obtained the worst score on this criterion.
The previous subparagraphs intended to give criteria descriptions and guidelines to evaluate alternatives according to them. The next section
will be devoted to specify the fuzzy weights relative to the criteria importance and establish the fuzzy decision matrix.
4.2.4
|
Fuzzy weighting of criteria
The weights describe the relative importance of each criterion with regard to the other criteria. Within fuzzy context, criteria importances are
expressed using linguistic terms and TFNs as seen in Table 6. According to the agility values and principles, a priority is given to the organization
and technique categories compared with modeling and SQA. Besides, some criteria are preferred than others, reflecting in this way the DM pref-
erences with regard to the agility fundamentals. For example, client implication (O1) and cadence of short development cycles (T2) are favored to
documentation volume (T5). The reason behind is to highlight the importance given in agility to the client involvement throughout the SDLC and
its satisfaction by means of quick realization and delivery, compared with the documentation.
4.3
|
Establishing fuzzy decision matrix
The fuzzy decision matrix shown in Table 7 contains the overall fuzzy performance rating of alternative according to criteria and their associated
fuzzy weights. Those fuzzy values are determined according to the guidelines and linguistic terms previously defined. It is important to note that
an agile method could have a good evaluation in one criterion, appears less valuable regarding their performance on one or more other criteria.
TABLE 6
Linguistic scales and corresponding triangular fuzzy numbers for the criteria weighting
Linguistic Scale of ImportanceTriangular Fuzzy Scale
CriterionCrisp Weight
Very high (VH)High (H)(0.75,1,1)O1,O5, T1, T2, T3, Q20.92
(0.50,0.75,1)T4, T7, M1, Q10.75
Medium (M)(0.25,0.50, 0.75)O2, O3, O6, T5, Q30.50
Low (L)(0.0,0.25, 0.50)O4, T6, M20.25
Very low (VL)(0,0,0.25)M3, Q40.08EL BEGGAR
TABLE 7
15 of 21
Fuzzy decision matrix for performance rating of agile methods
Fuzzy Rating of Alternatives
CriteriaScrumXP
Crystal Clear
FDD
ASD
AMDD
O1GVGGGGG
O2LMVSLLS
O3FGVGVGFVP
Kanban
DSDMFuzzy Weight
GGVH
SLM
VPVGM
O4FFFFVGFFFL
O5GVGFGGPPGVH
O6MMVSLLLLLM
T1VGVGVGVGVGVGFVGVH
T2MVLVSVLSMMMVH
T3FVGFFFVGVGFVH
T4VGFVGFVGFFVGH
T5MMMLMMMLM
T6FVGFPFVGFFL
T7MMMLMMMMMMSVLH
M1FFFGFVGFGH
M2MMMLMVLMVLL
M3FFFGFVGFFVL
Q1PVGPPPVGPPH
Q2MVLMMMVSMMVH
Q3GGGGGVPVGGM
Q4VPGVGVPVPGVPVPVL
Based on the decision matrix, the next step consists of calculating fuzzy aggregated preference indices and fuzzy outranking flows to deduce the
resulting partial and complete outranking that will be discussed in the next section.
5
5.1
|
RESULTS AND DISCUSSION
|
Outranking of agile methods
Based on the fuzzy performance rating of alternatives in the previous decision matrix and according to the Equation 21, the fuzzy preference
e k ða; bÞ as shown in Table 8. Using
ek ða; bÞ are calculated. Thereafter, Equation 22 permits calculating the fuzzy preference indices Π
degrees P
þ
−
e , the fuzzy entering flows ϕ
e ;and the fuzzy net flows ϕ
e can be computed respectively as seen
Equations 23, 24, and 25, the fuzzy leaving flows ϕ
in Table 9. Hence, by defuzzifing those flows, the partial order of the studied agile methods using PROMETHEE I and linear preference function
reveals certain incomparability and preference between alternative agile methods. For instance, the following methods are incomparable (SCRUM
R(I) AMDD, ASD R(I) AMDD, and Kanban R(I) Crystal Clear). Meanwhile, XP is preferred to DSDM and likewise DSDM is preferred to FDD (XP P(I)
DSMDM and DSDM P(I) FDD). The overall partial ranking performed by PROMETHEE I is shown in the Figure 5. On the other hand, the resulting
crisp net flows which are ϕ (XP) = 0.26, ϕ (DSDM) = 0.20, ϕ (FDD) = 0.11, ϕ (ASD) = 0.00, ϕ (Scrum) = ‐0.04, ϕ (AMDD) = ‐0.08, ϕ (Crystal
C) = ‐0.21 and ϕ (Kanban) = ‐0.24 allow a complete ranking via PROMETHEE II, which is XP P(II) DSDM P(II) FDD P(II) ASD P(II) Scrum P(II) AMDD
P(II) Crystal Clear P(II) Kanban. Consequently, the favorite equilibrium or compromise among all criteria is reached by the XP method (cf Figure 6).
For a deep performance analysis, calculating net flows by criteria group allows comparing agile methods against the different project views.
5.2
|
Performance analysis by criteria group
As stated by Brans and Vincke,18 the PROMETHEE II allows finding the best compromise achieved by an alternative on all criteria by combining
together its own positive and negative flows (see Figure 7). Within our study context, however, a deep analysis concerning these agile methods
facing each group of criteria independently of the other groups has to be conducted. Our aim is to provide in‐depth comparisons of agile methods
with regard to each IT project viewpoint. To do that, the indices and flows are recalculated for each group (organization, technique, modeling, and
SQA), with regard to the associated rating and weights of each group. In fact, the appropriateness analysis of the agile methods with regard to
criteria groups shows that XP could be chosen for project with great emphasis on technical view and effective software quality (ϕT (XP) ≅
0.28, ϕSQA (XP) ≅0.68). The analysis confirms further that XP does not present high performances against project management and modeling
due to its nature focused on code implementation (ϕO (XP) ≅ 0.15, ϕM (XP) ≅ −0.30). Nevertheless, XP offers many engineering advantages that
make it reasonable enough to be applied for several projects, such as quick code production and review, great acceptance of requirementsEL BEGGAR
16 of 21
TABLE 8
Fuzzy preference indices
ab
ScrumXP
CC
FDD
ASD
AMDD
Kanban
DSDM
XP
e k ða; bÞ
Π
e k ða; bÞ
Π
ab(0.0323, 0.1020,0.2540)
(0.1613, 0.2449, 0.5794)
(0.0323, 0.0816, 0.4206)
(0.0000,0.0816, 0.4286)
(0.3226, 0.3469, 0.4365)
(0.2419, 0.3061, 0.6270)
(0.0000,0.0000, 0.3175)ASDXP
CC
FDD
Scrum
AMDD
Kanban
DSDM(0.0323, 01633, 0.2937)
(0.0645, 0.2959, 0.5952)
(0.0323, 0.1327, 0.4603)
(0.0000, 0.0918, 0.5079)
(0.3226, 0.3673, 0.4762)
(0.2419, 0.3878, 0.6349)
(0.0000, 0.0204, 0.3254)
Scrum
CC
FDD
ASD
AMDD
Kanban
DSDM(0.2097, 0.4286, 0.6190)
(0.3710, 0.5510, 0.6587)
(0.1613, 0.3061, 0.4365)
(0.2581, 0.3673, 0.5476)
(0.3065, 0.3673, 0.5079)
(0.3710, 0.5510, 0.6825)
(0.2097, 0.3265, 0.4127)AMDDXP
CC
FDD
ASD
Scrum
Kanban
DSDM(0.0323, 0.1224, 0.2381)
(0.2742, 0.4388, 0.5635)
(0.1129, 0.1939, 0.3968)
(0.1452, 0.3265, 0.4603)
(0.1452, 0.3163, 0.5000)
(0.1452, 0.3061, 0.6429)
(0.1129, 0.1633, 0.3254)
CCXP
Scrum
FDD
ASD
AMDD
Kanban
DSDM(0.0323, 0.0612, 0.2143)
(0.0161, 0.0408, 0.4206)
(0.0323, 0.0816, 0.3730)
(0.0161, 0.0408, 0.3413)
(0.1935, 0.3061, 0.3651)
(0.1129, 0.2653, 0.5556)
(0.0000, 0.0000, 0.2381)KanbanXP
CC
FDD
ASD
AMDD
Scrum
DSDM(0.0000,0.0408, 0.1984)
(0.1744, 0.2449, 0.4841)
(0.0484, 0.1020, 0.3254)
(0.0484, 0.1633, 0.4080)
(0.1290, 0.1224, 0.4048)
(0.0484, 0.1224, 0.4127)
(0.0484, 0.0816, 0.3175)
FDDXP
CC
Scrum
ASD
AMDD
Kanban
DSDM(0.0000, 0.2041.0, 3492)
(0.1613, 0.3673, 0.6429)
(0.0645, 0.2857, 0.6429)
(0.1129, 0.2449, 0.5397)
(0.3387, 0.4082, 0.4841)
(0.2581, 0.4490, 0.7143)
(0.0484, 0.0816, 0.4206)DSDMXP
CC
FDD
ASD
AMDD
Kanban
DSDM(0.0323, 0.3265, 0.4365)
(0.2258, 0.4286, 0.6508)
(0.0968, 0.1429, 0.5159)
(0.0484, 0.3061, 0.6111)
(0.3548, 0.4490, 0.5476)
(0.3065, 0.4898, 0.7222)
(0.0806, 0.2653, 0.6190)
TABLE 9The resulting fuzzy flowsRankMethod
1XP
23
e þ)
Fuzzy Leaving Flow (ϕ
e −)
Fuzzy Entering Flow (ϕ
e
Fuzzy Net Flow (ϕ)Crisp Net Flow (ϕ)
(0.2696, 0.4140, 0.5567)(0.0230, 0.1458, 0.2834)(‐0.0139, 0.2682, 0.5336)0.26
DSDM(0.1636, 0.3440, 0.5862)(0.0599, 0.0962, 0.3367)(‐0.1731, 0.2478, 0.5263)0.20
FDD(0.1406, 0.2915, 0.5420)(0.0737, 0.1487, 0.4184)(‐0.2778, 0.1429, 0.4682)0.11
4ASD(0.0991, 0.2085, 0.4705)(0.0899, 0.2187, 0.4807)(‐0.3816, ‐0.0102, 0.3807)0.00
5Scrum(0.1129, 0.1662, 0.4376)(0.0922, 0.2216, 0.5306)(‐0.4177, ‐0.0544, 0.3455)‐0.04
6AMDD(0.1382, 0.2668, 0.4467)(0.2811, 0.3382, 0.4603)(‐0.3221, ‐0,0714, 0.1656)‐0.08
7CC(0.0576, 0.1137, 0.3583)(0.2051, 0.3673, 0.5964)(‐0.5388, ‐0.2536, 0.1532)‐0.21
8Kanban(0.0714, 0.1254, 0.3639)(0.2396, 0.3936,0.6542)(‐0.5828, ‐0.2682, 0.1243)‐0.24
FIGURE 5Partial ranking of agile methods using the linear functionFIGURE 6Complete ranking of agile methods using the linear functionEL BEGGAR
FIGURE 7
17 of 21
Comparison of resulting defuzzified flows of agile methods
changes, promoting communication and process improvement, etc. DSDM and FDD could be considered potential candidates of lightweight
methodologies, especially for large projects. (ϕT (DSDM) ≅ 0.22, ϕO (DSDM) ≅ 0.28, ϕM (DSDM) ≅ 0.42, ϕSQA (DSDM) ≅ −0.07, ϕT (FDD) ≅ −0.01,
ϕO (FDD) ≅ 0.28, ϕM (FDD) ≅ 0.45, ϕSQA (FDD) ≅ −0.07). We can observe besides that DSDM ISQAðIIÞ FDD and DSDM IOðIIÞ FDD, ie, those methods
are indifferent regarding SQA and organization views, since they devoted somewhat the same degree of interest for them. Compared with XP,
they present also more performance on the modeling viewpoint, while XP is more effective on the technique and SQA viewpoints, as we can
see in the radar schema concerning the top three agile methods (cf, Figure 8).
On the other hand, the methods Scrum, ASD, and AMDD could be considered as single project view driven processes. Indeed, Scrum and ASD
are more focused on project management or organization with respectively net flows ϕO (Scrum) ≅ 0.1 and ϕO (ASD) ≅ 0.24, while AMDD is
centered on modeling, ϕM (AMDD) ≅ 0.64. They get thus best relative ranking according to their preoccupations. Kanban in turn is more about
continuous flows than sprinting.2 It is more interested in process instead of iteration achievement every few weeks. Kanban obtained hence
low performances overall criteria groups, except for the SQA group (ϕSQA (Kanban) ≅ −0.03). Indeed, Kanban got a good score on the criterion
“implicit improvement of approval process.” Finally, despite the hopeful performances obtained by Crystal Clear according to SQA (ϕSQA
(Crystal Clear) ≅ −0.07), it suffers from lack of practices related to technical and organizational axes. It is more tailored for very small projects
and teams gathered in the same space, with an ambient communication. Other method of Crystal family might compensate this lack. Figure 9
allows a full comparison of agile methods performances against the criteria groups.
FIGURE 8Comparison of the top three agile methods by criteria group
FIGURE 9Comparison of net flows of the studied agile methods by criteria groupsEL BEGGAR
18 of 21
In light of the obtained results, the applied MCDA approach confirms that there is more compliance of XP, DSDM, and FDD with the agility
values and principles, since their associated practices and activities fulfill this purpose. However, the latter top three methods suffer from some
insufficiencies to reach excellent performances overall the studied criteria. On the other hand, the rest of the agile methods lack of some evi-
dences to obtain better performances, probably due to their nature of being ultra‐adaptive processes rather than prescriptive ones that guide pro-
ject managers throughout the SDLC. As synthesis, specific merge or unification between some of agile methods might create new efficient hybrid
processes regarding all IT project viewpoints.
5.3
|
Sensitivity analysis
This section is dedicated to assess how different values of criteria weights impact the ranking of alternatives obtained beforehand. The objective is
to examine the effect of changing criteria weights on the stability or robustness of final results.62 Commonly, when applying sensitivity analysis,
usually, we change the weight of one criterion, while the other criteria weights remain invariable. For that, the sensitivity analysis is conducted by
slightly varying the values of the criterion weights from their original linguistic terms to the other terms predefined previously in Table 6, and then
observing the impact on the decision (alternatives ranking). Therefore, Table 10 shows weights intervals [Wmin; Wmax] expressed in linguistic terms
which determine criterion limits to be varied without changing the resulting ranking obtained by F‐PROMETHEE.
According to the values of the Table 10, we can conclude that the variation of some criteria weights, such as O1, O2, O4, T5, T6, T7, M1, M3, Q3
and Q4 cannot affect the complete ranking.
Meanwhile, the criteria T1, T2, T3 T4 M2, Q1, and Q2 have the considerable impact on the complete ranking. In fact, weight variation of criteria
T1, T2, and M1 do not influence the top three ranking, they alter just the ranking of Kanban and Crystal Clear that exchange their respective places
in the original complete ranking or AMDD and Scrum which do same. Only weight variation of criteria T2, T3, T4, M2, Q1, and Q2 impact directly the
top three ranked methods without affecting the remainder ranking. Indeed, FDD dominates DSDM for criterion T3, while DSDM dominates XP for
the rest of criteria. Therefore, we can say that the evaluation is relatively insensitive to criteria weights; however, when the weights of technical
criteria (T1, T2, and T4) or quality criteria (Q1 and Q2) are set at the lowest term, then the best solution is changed from XP to DSDM, because of
the highest performances obtained by XP against technique and SQA criteria.
5.4
|
Limitations of our approach
The main challenge in the majority of the MCDA approaches is the assignment of criteria weights and the scoring of alternatives. Providing accurate
scoring of alternatives in our case is often hard, since the alternative agile methods being assessed are generally object of improvement over time. In
other words, they could integrate other practices or activities susceptible for a probable increasing or decreasing of their performances. There is also
TABLE 10
Weighting intervals keeping the stability of agile methods' complete ranking
Weighting Interval (Using Linguistic Terms)
CriterionOriginalWminWmax
O1VHVLVH
O2MVLVH
O3MLVH
O4LVLVH
O5VHLVH
O6MVLH
T1VHMVH
T2VHMVH
T3VHMVH
T4HMVH
T5MVLVH
T6LVLVH
T7HVLVH
M1HVLVH
M2LVLM
M3VLVLVH
Q1HMVH
Q2VHMVH
Q3MVLVH
Q4VLVLVHEL BEGGAR
19 of 21
uncertainty and lack of consensus concerning alternatives scoring, because some practices and activities performed by agile methods are not defi-
nitely known or decided. The first effort is deployed based on a wide and filtered review of literature to gather data that serve as an evaluation guid-
ance of the studied agile methods. Next, fuzzy scales using linguistic terms and TFNs are used to score alternatives and weigh criteria instead of crisp
numbers to deal with uncertainty. On the other hand, our approach has attempted to deal with the DMs subjectivity by integrating preference func-
tions and associated indifference and preference thresholds. The latter allow modeling realistically the DMs preferences on every specific criterion.
However, as stated by Hyde et al,63 DMs have much difficulty in choosing the appropriate preference functions and their associated thresholds. In
fact, although the PROMETHEE method supports six preference functions, the proposal adopted the linear preference function for all criteria to make
preference between alternatives performances from each other. The study further adopted respectively the indifference and preference thresholds
0.1 and 0.3. However, other preference functions and thresholds values could be selected and tested in this topic.
The utilization of fuzzy logic did not address completely the uncertainty in the DMs judgments. The weighting of the criteria is another critical
input parameter, which increases uncertainty and subjectivity in such decision making problems. In PROMETHEE, there is no standard method or
technique to assign the weights to criteria. However, the weight of a criterion is defined according to whether this criterion is more important or
less important than other criteria in the philosophy of agility. Nevertheless, incorporate additional methods to PROMETHEE, such as Swing,6
AHP,64 and ROC,65 could help DMs to set the criteria weights in a user‐friendly way. Furthermore, when multiple DMs are invited to weigh
criteria, this aspect is not treated in our proposal; usually, it is difficult to make a common accord on the criteria importance. An average of the
criteria weights could be calculated, but in this case, the importance of a dimension value will be decreased more than it deserves.
To deal with this hurdle, a sensitivity analysis is required at the final stage of the decision‐making problem.62 In our proposal, a sensitivity anal-
ysis is performed on the weights of criteria to verify the stability and robustness of the final results. Meanwhile, a sensitivity analysis concerning
preference functions and associated thresholds values could be also conducted to assess their impact on the ranking of agile methods.
Another limitation of our proposal when other agile methods are added to the evaluation; the DMs should start anew the pair comparisons,
and recalculate again the resulting flows to make the outranking.
6
|
C O N CL U S I O N
The present paper proposed an MCDA approach to study the appropriateness of agile methods with regard to the project environment. This eval-
uation is based on the comparison of agile methods according to different conflicting criteria, divided into four groups each one related to a project
view, which are organization, technique, modeling, and SQA. The proposed MCDA approach is based on the F‐PROMETHEE method; in fact, the
fuzzy logic is integrated to PROMETHEE to deal with uncertainty and vagueness of both rating performance of alternatives and criteria weighting.
The evaluation process performed by F‐PROMETHEE concerns eight alternative lightweight methods, which are AMDD, ASD, Crystal Clear,
DSDM, FDD, Kanban, Scrum, and XP that are assessed against 20 criteria. The first processed step in our proposal aims to determine guidelines
and linguistic terms to establish the fuzzy decision matrix, while the second step concerns computation of aggregated preference indices and
different flows. The resulting flows are defuzzified afterward to perform partial and complete ranking in the last step.
The obtained results show that the optimal balance among all criteria is achieved by XP that presented the better performances on these
criteria compared with the other methods. This fact makes it the suitable choice to be applied in several projects. With a degree less, DSDM
and FDD positioned after. Indeed, those top three methods presented more relevant practices and activities in concordance with the agility prin-
ciples throughout the SDLC. On the other hand, despite the popularity of some agile methods, the study confirms the lack of practices or tech-
niques that allow stating that they could be efficient for all IT project views. Instead, they could be used as guidelines methods to manage partially
a single or several IT project aspects; reinforced them by other methods to handle the remainder aspects could be a judicious choice.
As there are no unification of agile methods or, on the other hand, an agile method eligible for any type of project, any research of this sort would be
of great interest for academics and practitioners of industry alike, enabling them to decide when to use an agile method and not some others. Besides,
the project managers could use our study from beginning at the feasibility stage, as a framework to select the best‐fit method to be adapted for their
project. The project managers might simply choose the methods to be evaluated and scoring their performance according to the defined criteria. The
applicability of such issue still remains to be done in real situations, as well as a CASE tool to support our approach will represent a high added value.
In the future works, more agile methods will be added to extend evaluation. We also plan to propose a unified trade‐off of agile methods,
gathering the best practices and techniques that cover all project viewpoints and provide thus a coalescence non coalition of those lightweight
methods, since agile alliance existed beforehand.
ORCID
Omar El Beggar
http://orcid.org/0000-0002-5122-3322
RE FE R ENC E S
1. Pawar RP. A comparative study of agile software development methodology and traditional waterfall model. IOSR J Comp Eng (IOSR‐JCE). 2015;1‐8.
2. Flora HK, Chande SV. A systematic study on agile software development methodologies and practices. Intern J Comp Sci Inform Technol (IJCSIT).
2014;5(3):3626‐3637.20 of 21
EL BEGGAR
3. Almseidin M, Alrfou K, Alnidami N, Tarawneh A. A comparative study of agile methods: XP versus SCRUM. Intern J Comp Sci Softw Eng (IJCSSE).
2015;4(5):126‐129.
4. Abrahamson P, Salo O, Ronkainen J, Warsta J. Agile software development methods: Review and analysis, VTT publication 478. Finland: Espo; 2002:107.
5. Agile Manifesto: http://agilemanifesto.org/, Accessed 20/02/2018
6. Silva VBS, Schramm F, Damasceno AC. A multicriteria approach for selection of agile methodologies in software development projects. 2016 IEEE Int
Conf on Systems, Man, and Cybernetics SMC, October 9‐12, 2016, Budapest, Hungary, pp. 2056‐2060/
7. Hesari S, Mashayekhi H, Ramsin R. Towards a general framework for evaluating software development methodologies. In IEEE Computer Software and
Applications Conference, Seoul, South Korea, July 2010, pp. 208‐217.
8. Taromirad M, Ramsin R. CEFAM: Comprehensive evaluation framework for agile methodologies. In IEEE Annual Software Engineering Workshop,
Kassandra, Greece, October 2008, pp. 195‐204.
9. Peng Y, Zhang Y, Kou G, Li J, Shi Y. Multicriteria decision making approach for cluster validation. International Conference on Computational Science,
ICCS 2012, Elsevier, Procedia Computer Science 9, pp. 1283 – 1291
10. Liao H, Xu Z. Multi‐criteria decision making with intuitionistic fuzzy PROMETHEE. J Intell Fuzzy Syst. 2014;27:1703‐1717.
11. Zardari NH et al. Weighting methods and their effects on multi‐criteria decision making model outcomes in water resources management. SpringerBriefs
Water Sci Technol. 2015;07‐67.
12. Mergias I, Moustakas K, Papadopoulos A, Loizidou M. Multi‐criteria decision aid approach for the selection of the best compromise management
scheme for ELVs: the case of Cyprus. J Hazard Mater. 2007;147(3):706‐717.
13. Hanine M, Boutkhoum O, Tikniouine A, Agouti T. A new web‐based framework development for fuzzy multi‐criteria group decision‐making.
Springerplus. 2016;5(601):1‐18.
14. Keeney RL, Raiffa H. Decision with Multiple Objectives: Preference and Value Trade‐offs. New York: Wiley; 1976.
15. Saaty TL. The Analytic Hierarchy Process. New York: McGraw‐Hill; 1980.
16. Edwards W, Barron FH. SMARTS and SMARTER: improved simple methods for multiattribute utility measurement. Organ Behav Hum Decis Process.
1994;60(3):306‐325.
17. Banae Costa CA, Corte JM, Vansnick JC. On the mathematical foundations of MACBETH. In: Figueira J, Greco S, Ehrgott M, eds. Multiple criteria
decision analysis: state of the art surveys. New York: Springer; 2005.
18. Brans JP, Vincke P. A preference ranking organization method (the PROMETHEE method for multiple criteria decision‐making). Manag Sci.
1985;31(6):647‐656.
19. Figueira J, Mousseau V, Roy B. ELECTRE Methods, in: Multiple Criteria Decision Analysis: State of the Art Surveys. Boston, Dordrecht, London: Springer‐
Verlag; 2005:133‐162.
20. Guarini MR, Battisti F, Chiovitti A. A methodology for the selection of multi‐criteria decision analysis methods in real estate and land management
processes. Sustainability. 2018;10(2), 507):1‐28.
21. Afful‐Dadzie E, Nabareseh S, Oplatková ZK, Klimek P. Using Fuzzy PROMETHEE to Select Countries for Developmental Aid. In: Bi Y et al., eds.
Intelligent Systems and Applications, Studies in Computational Intelligence. Springer International Publishing Switzerland; 2016:109‐132.
22. Ying‐Hsiu C, Tien‐Chin W, Chao‐Yen W. Strategic decisions using the fuzzy PROMETHEE for IS outsourcing. Expert Syst Appl.
2011;38(10):13216‐13222.
23. Amaral TM, Ana PCC. Improving decision‐making and management of hospital resources: an application of the PROMETHEE II method in an
Emergency Department. Operat Res Health Care. 2014;3(1):1‐6.
24. Gupta R, Sachdeva A, Bhardwaj A. Selection of logistic service provider using fuzzy PROMETHEE for a cement industry. J Manuf Technol Manag.
2012;23(7):899‐921.
25. Elevli B. Logistics freight center locations decision by using fuzzy‐PROMETHEE. Transport. 2014;29(4):412‐418.
26. Tuzkaya G, Gülsün B, Kahraman C, Özgen D. An integrated fuzzy multi‐criteria decision making methodology for material handling equipment selection
problem and an application. Expert Syst Appl. 2010;37(4):2853‐2863.
27. Celik E, Gul M, Aydin N, Gumus AT, Guneri AF. A comprehensive review of multi criteria decision making approaches based on interval type‐2 fuzzy
sets. Knowl‐Based Syst. 2015;85:329‐341.
28. Shelly. Comparative analysis of different agile methodologies. Intern J Comp Sci Inform Technol Res. 2015;3(1):199‐203.
29. Kumari U, Upadhyaya A. Comparative study of agile methods and their comparison with heavyweight methods in Indian organizations. Intern J Recent
Res Rev. 2013;VI:4‐11.
30. Moniruzzaman ABM, Hossain SA. Comparative study on agile software development methodologies. Global J Comp Sci Technol Softw Data Eng.
2013;13(7):4‐18.
31. El Mehdi A, Yassin CM, Eddine EKK. Survey and Comparative Study on Agile Methods in Software Engineering. Transactions on Machine Learning and
Artificial Intelligence, Special Issue August, 2017, pp. 333‐345
32. Kumar G, Bhatia PK. Impact of agile methodology on software development process. Intern J Comp Technol Electr Eng (IJCTEE). 2012;2(4):46‐50.
33. Sharma S. Agile processes and methodologies: a conceptual study. Intern J Comp Sci Eng (IJCSE). 2012;4(5):892‐898.
34. Qureshi MRJ. Agile software development methodology for medium and large projects. IET Softw. 2012;6(4):358‐363.
35. Adelyar SH, Norta A. Towards a Secure Agile Software Development Process. 10th Int IEEE Conference on the Quality of Information and Communi-
cations Technology (QUATIC), 2016, pp. 101–106.
36. Mahmud DM, Abdullah NAS. Reviews on agile methods in mobile application development process. In 9th IEEE Malaysian Software Engineering
Conference (MySEC), 2015 pp 161–165.
37. Kropp M, Meier A. Collaboration and human factors in software development: teaching agile methodologies based on industrial insight. In Global
Engineering Education Conference (EDUCON), IEEE, April 2016, pp. 1003‐1011.EL BEGGAR
21 of 21
38. Dyck S, Majchrzak TA. Identifying common characteristics in fundamental, integrated, and agile software development methodologies. 45th Hawaii
International Conference on System Science (HICSS), IEEE, January 2012, pp. 5299‐5308.
39. Harb YA, Noteboom C, Sarnikar S. Evaluating project characteristics for selecting the best‐fit agile software development methodology: a teaching case
evaluating project characteristics for selecting the best‐fit. J Midwest Assoc Inform Syst. 2015;1(1):33‐52.
40. Sarfaraz A.H., Maleki H. Intransitivity in inconsistent judgments. In: Katarzyniak R., Chiu TF., Hong CF., Nguyen N.T. (eds) Semantic Methods for
Knowledge Management and Communication. Studies in Computational Intelligence, vol 381. Springer, Berlin, Heidelberg, 2011, pp. 81‐89.
41. Demirtas N, Tuzkaya UR, Seker S. Project Management Methodology Using SWOT‐Fuzzy AHP. In 2014 World Congress on Engineering, London,
United Kingdom, July 2014, pp. 1121‐1126.
42. Vavpotič D, Vasilecas O. An approach for assessment of software development methodologies suitability. Electr Electr Eng. 2011;114(8):107‐110.
43. Gul M, Celik E, Gumus AT, Guneri AF. A fuzzy logic based PROMETHEE method for material selection problems. Beni‐Suef Univ J Basic Appl Sci.
2018;7(1):68‐79.
44. Ozsahina DU et al. Evaluating nuclear medicine imaging devices using fuzzy PROMETHEE method, 9th Int Conference on Theory and Application of
Soft Computing, Computing with Words and Perception, ICSCCW 2017, Budapest, Hungary. Procedia Comp Sci. 2017;120:699‐705.
45. Pratibha Rani and Divya Jain. Intuitionistic fuzzy PROMETHEE technique for multi‐criteria decision making problems based on entropy measure. (eds)
Advances in Computing and Data Sciences. ICACDS 2016. Communications in Computer and Information Science, Springer, Singapore, Vol 721, 2017
46. Tabaraee E, Ebrahimnejad S, Bamdad S. Evaluation of power plants to prioritise the investment projects using fuzzy PROMETHEE method. Taylor &
Francis Intern J Sustain Energy. 1‐14. https://doi.org/10.1080/14786451.2017.1366489
47. Lolli F, Ishizaka A, Gamberini R, et al. Waste treatment: an environmental, economic and social analysis with a new group fuzzy PROMETHEE approach.
Clean Techn Environ Policy. 2016;18(5):1317‐1332.
48. Elevli B. Logistics freight center locations decision by using fuzzy‐PROMETHEE. Taylor & Francis J Transp. 2014;29(4):412‐418.
49. Boutkhoum O, Hanine M, Agouti T, Tikniouine A. Selection problem of cloud solution for big data accessing: fuzzy AHPPROMETHEE as a proposed
methodology. J Digit Inf Manag. 2016;14(6):368‐382.
50. Sharma A, Bawa DRK. Modified fuzzy PROMETHEE approach for agile method selection using fuzzy AHP. I J C T A. 2016;9(41):641‐649.
51. Marsh K, IJzerman M, Thokala P, et al. Multiple criteria decision analysis for health care decision making: report 2 of the ISPOR MCDA emerging good
practices task force. Value Health. 2016;19(2):125‐137.
52. Annual report of agile survey: https://explore.versionone.com/state‐of‐agile/versionone‐11th‐annual‐state‐of‐agile‐report‐2, Accessed at 01/03/
2018.
53. Basili VR. Software Modeling And Measurement: the Goal/Question/Metric Paradigm. College Park: University of Maryland; 1992:1‐24.
54. Agile alliance practices: https://www.agilealliance.org/agile101/practices‐timeline/, Accessed 27/06/2018
55. Qumer A, Henderson‐Sellers B. An evaluation of the degree of agility in six agile methods and its applicability for method engineering. Inf Softw Technol.
2008;50(4):280‐295.
56. SenthilMurugan C, Prakasam S. A literal review of software quality assurance. Int J Comput Appl. 2013;78(8):25‐30.
57. Ambler SW. Agile Model Driven Development (AMDD), XOOTIC MAGAZINE, February 2007, pp. 13–25.
58. Walczak W, Kuchta D. Risks characteristic of agile project management methodologies and responses to them. Operat Res Dec. 2013;4:75‐95.
59. Abrahamsson P, Warsta J, Siponen MT, Ronkainen J. New Directions on Agile Methods: A Comparative Analysis', 25th international conference on Soft-
ware Engineering (ICSE), IEEE, 2003, pp. 244–254.
60. OMG: model driven architecture (MDA). 2004. http://www.omg.org/cgi‐bin/doc?formal/03‐06‐01.
61. Ambler SW. Agile model driven is good enough. IEEE Softw. 2003;20(5):71‐73.
62. Ishizaka A, Nemery P. Multi‐Criteria Decision Analysis Methods and Software. John Wiley & Sons, Ltd; 2013.
63. Hyde K, Maier HR, Colby C. Incorporating Uncertainty in the PROMETHEE MCDA Method. J Multicrit Decis Anal. 2003;12(4‐5):245‐259.
64. Dagdeviren M. Decision making in equipment selection: an integrated approach with AHP and PROMETHEE. J Intell Manuf. 2008;19(4):397‐406.
65. Morais DC, Almeida AT, Alencar LH, Clemente TRN, Cavalcanti CZB. PROMETHEE‐ROC model for assessing the readness of technology for generating
energy. Math Probl Eng. 2015;2015:1‐11.
How to cite this article: El Beggar O. Multicriteria decision aid for agile methods evaluation using fuzzy PROMETHEE. J Softw Evol Proc.
2018;30:e2108. https://doi.org/10.1002/smr.2108